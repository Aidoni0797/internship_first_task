4.8 Saving the result to Excel

CSV

CSV (Comma Separated Values) - значение, разделенные запятыми. Это один из самых простых способов хранения
информации в виде обычного текста. Данные представлены в табличной форме, где каждая строка является строкой 
запси таблицы. Модуль CSV входит в пакет поставки python, это означает, что нам не нужно его устанавливать,
достаточно сделать import csv.

Заказчикам на фриланс-биржах часто нужно сохранять результаты именно в один из популярных форматов 
CSV or JSON.

Формат CSV выглядит вот так: one; two; three, имеет расширение файла .csv и с легкостью открывается в Excel:
Именно вот так будет выглядеть наши данные, если мы откроем их в Excel. Работа с модулем CSV очень проста
и нам понадобится запомнить лишь пару приемов для того, чтобы успешно применять его на практике.

Для начала, нам нужно знать что любой список легко превратить в CSV

import csv

lst = ['one', 'two', 'three']

with open('res.csv', 'w', newline='', encoding='utf-8-sig') as file:
  writer = csv.writer(file, delimiter=';')
  writer.writerow(lst)

Чтобы в Excel корректно открывалась кодировка, используйте encoding='utf-8-sig'.
В результате выполнения этого кода будет создан файл res.csv, в который будет записан наш список 
lst, в каждой ячейке - элемент списка.

- newline='' - необходимо указывать всегда. Если не указать, то новые строки могут интерпретироваться
неправильно и весь документ "сползет"
- encoding = 'utf-8-sig' - open() использует для открытия .csv по умолчанию кодировки unicode. Чтобы
получить файл с необходимой нам кодировкой, нужно явно указывать ее.

witer = csv.writer(file, delimiter=';') - в этой строке мы создали экземпляр класса csv и применили к нему
метод writer(). У writer() есть метод writeow(), с помощью кторого можно записывать список в соответствующий
формат построчно. delimiter=';' указывает, каким будет разделить между элементами списка, мы можем
указатьт любой.

CSV часть 2

В этом уроке я хочу наглядно показать, как собрать информацию с тренажера (https://parsinger.ru/html/mouse/3/3_11.html)
и сохранить результат в формате .csv

import csv
import requests

from bs4 import BeautifulSoup

#1---------------------------------------------
with open('res.csv', 'w', encoding='utf-8-sig', newline='') as file:
  writer = csv.writer(file, delimiter=';')
  writer.writerow([
    'Наименование', 'Артикул', 'Бренд', 'Модель',
    'Тип','Игровая','Размер','Подсветка','Разрешение',
    'Сайт производителя','В наличии','Цена'
  ])
#1---------------------------------------------

#2---------------------------------------------
url = 'http://parsinger.ru/html/mouse/3/3_11.html'

response = requests.get(url=url)
response.encoding = 'utf-8'
soup = BeautifulSoup(response.text, 'lxml')
#2---------------------------------------------

#3---------------------------------------------
name = soup.find('p', id='p_header').text
article = soup.find('p', class_='article').text.split(': ')[1]
brand = soup.find('li', id='brand').text.split(': ')[1]
model = soup.find('li', id='model').text.split(': ')[1]
type = soup.find('li', id='type').text.split(': ')[1]
purpose = soup.find('li', id='purpose').text.split(': ')[1]
light = soup.find('li', id='light').text.split(': ')[1]
size = soup.find('li', id='size').text.split(': ')[1]
dpi = soup.find('li', id='dpi').text.split(': ')[1]
site = soup.find('li', id='site').text.split(': ')[1]
in_stock = soup.find('span', id='in_stock').text.split(': ')[1]
price = soup.find('span', id='price').text.split(' ')[0]
#3---------------------------------------------

#4---------------------------------------------
with open('res.csv', 'a', encoding='utf-8-sig', newline='') as file:
  writer = csv.writer(file, delimiter=';')
  writer.writerow([
    name, article, brand, model,
    type, purpose, light, size, dpi,
    site, in_stock, price
  ])
#4---------------------------------------------

Я визуально разделил код на 4 части.

1. В первом блоке мы создали файл res.csv и определили в нем первые 12 ячеек для заголовков. При
просмотре в текстовом редакторе это будет выглядеть вот так:
2. Когда мы откроем его в excel, это будет выглядеть вот так. Наличик заголовков это хорошая
практика, но не обязательная.
3. Вторая часть кода - это стандартные запросы к сайту, которые вы уже использовали при выполнении задач;
4. Третья часть хорошо показывает, что мы получаем нужные нам элементы и храним их в переменных,
которые в 4-м пунтке мы передаем в метод .writerow(). Обратите внимание на то, что мы передаем список;
5. И в результате выполнения 4-го блока кода мы получаем готовый файл .csv, в котором будут красиво
лежать наши данные.
Обратите внимание, что в 4-м блоке мы используем флаг 'a' для того, чтобы дописать созданный в первом
блоке файл. 'a' - 'append' - добавление в конец. Если флаг будет 'w' - 'write', то файл перезапишется заново и 
заголовки мы не увидим.

CSV часть 3

В этом уроке, мы будем получать данные сразу с восьми карточек одновременно. Как говорится,
следите за руками.

Мы хотим получить аккуратно отформатированные данные, лежащие в таблице excel, примерно вот так:

Откроем тренажер - https://parsinger.ru/html/index3_page_2.html и соберем с каждой карточки следующую
информацию как на скриншоте ниже:

Для начала посмотрим на труктулу данной карточки, а конкретно на теги <li>. Мы видим, что у всех
необходимых нам элементов есть то, за что можно зацепиться. У каждого элемента есть свой class='',
а у тегов <li> ничего нет, только родительский элемент <div class="description">. По нему мы и будем
получать наши элементы <li>, причем все и сразу.

Мы могли бы использовать код soup.find('div', class_='description').find_all('li'), т.е., попросили
бы наш интерпретатор найти блок div с классом description и в нем отыскать все теги <li>.
Это отличный подход для одной карточки, а у нас их восемь. Это значит, что нам нужно искать все теги 
<div> с классом description на странице одновременно.

Давайте посмотрим на крд целиком.

- В переменной name у нас будет храниться список наименований товаров;
- В переменной description у нас будет храниться список списков описания товаров, кторые находятся в 
<li>;
- В переменной price у нас будет храниться список цен товаров.

Наша задача - объединить эти 3 списка в 1 список или кортеж, избегя вложенных списков. Если мы просто
объединим эти три списка мы получим

Мы знаем, что модуль CSV может спокойно работать с вложенными спсиками. Но резльутат будет не тот, 
который мы ожидаем - посмотрите на картинке. Поэтому нам нужно извлечь вложенный список.

Блок №5 в коде ниже, решает эту задачу. В результате выполнения блока №5 мы получаем новый список 
result, который выглядит как нужно

Такой список мы можем спокойно записать в файл, не опасаясь, что таблица уедет. Передадим список в метод
.writerow(result) и полюбуемя результатом. Запустите данный код у себя в терминале.

UDP: Код ниже был немного упрощен и исправлен, удалено открытие файлового дескриптора в цикле,
что приводило к постоянному переоткрытию файла, а это для больших списков не очень хорошо. Теперь код 
выглядит короче и понятнее и не вызовет проблем в будущем.

import csv 
import requests
from bs4 import BeautifulSoup

#1-----------------------------------------------
with open('res.csv', 'w', encoding='utf-8-sig', newline='') as file:
  writer = csv.writer(file, delimiter=';')
  writer.writerow([
    'Наименование', 'Цена', 'Бренд', 'Тип', 'Подключение', 'Игровая'
  ])
#1-----------------------------------------------

#2-----------------------------------------------
url = 'http://parsinger.ru/html/index3_page_2.html'

response = requests.get(url=url)
response.encoding='utf-8'
soup = BeautifulSoup(response.text, 'lxml')
#2-----------------------------------------------

#3-----------------------------------------------
name = [x.text.strip() for x in soup.find_all('a', class_='name_item')]
description = [x.text.split('\n') for x in soup.find_all('div', class_='description')]
price = [x.text for x in soup.find_all('p', class_='price')]
#3-----------------------------------------------

#4-----------------------------------------------

for item, price, descr in zip(name, price, description):
  flatten = item, price, *[x.split(':')[1].strip() for x in descr if x]

  file = open('res.csv', 'a', encoding='utf-8-sig', newline='')
  writer = csv.writer(file, delimiter=';')
  writer.writerow(flatten)

file.close()
print('Файл res.csv создан')

Задание 1

Напишите код, который собирает данные в категории HDD (https://parsinger.ru/html/index4_page_1.html)
со всех 4х страниц и сохраняет всё в таблицу по примеру предыдущего степа.

Информация которую необходимо собрать.

Заголовки: Наименование, Бренд, Форм-фактор, Ёмкость, Объём буф. памяти, Цена

Пример заголовков

Решение

import requests
from bs4 import BeautifulSoup
import csv

# URL шаблон для 4 страниц
base_url = "https://parsinger.ru/html/index4_page_{}.html"
pages = [base_url.format(i) for i in range(1, 5)]

# Заголовки CSV
headers = ["Наименование", "Бренд", "Форм-фактор", "Ёмкость", "Объём буф. памяти", "Цена"]
data = []

# Сбор данных со всех страниц
for url in pages:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    items = soup.find_all('div', class_='item')

    for item in items:
        name = item.find('a', class_='name_item').text.strip()
        description = item.find_all('li')
        brand = description[0].text.split(': ')[1]
        form_factor = description[1].text.split(': ')[1]
        capacity = description[2].text.split(': ')[1]
        buffer = description[3].text.split(': ')[1]
        price = item.find('p', class_='price').text.strip().replace(' руб', '')

        data.append([name, brand, form_factor, capacity, buffer, price])

# Сохраняем в CSV-файл
with open('hdd_data.csv', 'w', encoding='utf-8-sig', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(headers)  # Пишем заголовки
    writer.writerows(data)    # Пишем данные

print("hdd_data.csv")

