3.3 The requests.get() Method

requests.get()

Запросы к серверу можно направлять с самыми разными целями. Но мы будем делать акцент в основном на .get() запросы.
Запросы .get() обычно предназначены только для чтения данных без их изменения. С помощью этого метода мы научимся получать HTML-код страницы, скачивать изображения,
музыку и видео.

Чтобы выполнить .get() запрос, используется команда requests.get()

Это такой же тип запроса, какой направляет на сервер ваш браузер, чтобы открыть вам интернет-страницу. Единственное отличие заключается в том, что библиотека Requests
не может выполнить рендеринг кода HTML, поэтому вы получите просто текст кода HTML. Но пролверьте, там есть все, что нам нужно.

Сайт https://httpbin.org/ создан специально для веб-разработчиков, чтобы они могли тестировать свой код, отправляя свои запросы. Этот сайт возвращает информацию по конкретному 
запросу. Его мы будем использовать весь блок изучения библиотеки requests.

import requests

response = requests.get(url='http://httpbin.org/')

print(type(response))

>>><class 'requests.models.Response'>

Давайте вместе выполним это запрос и посмотрим type() возвращаемого объекта. Как видим, мы получаем объект класса Response, который является экземпляром класса requests.

Надеюсь, от последней фразы вы не запутались. Попробую объяснить проще. В последнем примере переменная response стала экземпляром класса requests и в нее передали
атрибут = url

Мы можем передавать этому классу и другие атрибуты, поднобнее о которых вы узнаете далее.

url - передает ссылку - цель, куда будет отправлен запрос. (Обязатлеьный)
params - словарь или байты, которые будут отправлены в строке запроса. (Необязательный)
headers - словарь HTTP-заголовков, отправляемых с запросом. (Необязательный)
cookies - объект Dict or CookieJar для отправки с запросом. (Необязательный)
auth - AuthObject для включения базовой аутентификации HTTP. (Необязательный)
timeout - Число с плавающей запятой, описывающее тайм-аут запроса. (Необязательный)
allow-redirects - логическое значение. Установите значение True, если разрешено перенаправление. (Необязательный)
proxies - Протокол соспоставления словаря с URL-адресом прокси. (Необязательный)
stream  - Удерживает соединение открытым, пока не получен весь Response.content (Необязательный)

Заголовки headers = 

Заголовки отправляются вместе с запросом и возврщаются с ответом. Они нужны, чтобы клиент и сервер понимали, как интерпритировать данные, отправляемые и
получаемые в запросе и ответе.

При написании парсеров рекомендуется отправлять свои заголовки вместе с .get() запросом, потому что сервер понимает, что делать с нашим запросом, именно исходя из тех
заголовков, которые мы отправили. Иногда необходимо маскироваться под браузер, подменять cookie, user-agent, headers, ip и др.

Вот пример отправляемого нашим скриптом заголовка.

import requests

response = resuets.get(url='http://httpbin.org/user-agent')
print(response.text)

>>>{"user-agent": "python-requests/2.27.1"}

Если сайт простой и программистов не заботят такие мелочи, то мы тоже не переживать и отправлять такие запросы. Но чаще всего это не так.

user-agent:Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTMK, like Gecko)
Chrome/99.0.4844.84 Safari/537.36

Для того, чтобы замаскировать свой запрос под запрос браузера, будем использовать .get() запрос с именованным атрибутом и передадим в него словарь headers.

import requests

headers = {
'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)
Chrome/99.0.4844.84 Safari/537.36',
}

response = requests.get(url='http://httpbin.org/user-agent', headers=headers)
print(response.text)

>>>{"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)
Chrome/99.0.4844.84 safari/537.36"}

В ответе на ваш запрос мы увидим переданный нами user-agent. Поэкспериментируйте самостоятельно, измените содержания словаря headers и посмотрите, что изменится.

Загрузите файт с user-agent и поместите его в корневой каталог с вашим проектом. Скачять файл - https://drive.google.com/file/d/1mIG_570jp_NSlPgeyCF2xOOZLjP2V82w/view

import requests
from ramdom import choice

url = 'http://httpbin.org/user-agent'

while line := open ('user-agent.txt').read().split('\n'):
  user_agent = {'user-agent': choice(line)}
  response = requests.get(url=url, header=user_agent)
  print(response.text)

>>>{"user-agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)
Chrome/50.0.2661.94 Safari/537.36"}
.....
Этот код последовательно представляет user-agent из файла и делает запрос на наш url. Обратите внимание, что в этом
примере использовался модуль random и его метод choice для случайного выбора user_agent из файла.

Когда вы будете пистаь свой первый парсер, вы можете использовать первый пример, т.к. он намного проще и нужен чаще всего.
Второй пример более сложен, такое необходимо для высокочастотных апрсеров и в тех случаях, когда сервер банит за спам-запросы.
В таком случае можно использовать задержки time.sleep(10).


Библиотека fake_useragent

Библиотека fake_useragent создана, чтобы облегчить нам жизнь. Она делает то же самое, что и код из прошлого степа, только еще проще.

Установка библиотеки
pip install fake-useragent
or
pip3 install fake-useragent

импорт

from fake_useragent import UserAgent
or
import fake_useragent

Экземпляр класса создается двумя способами. Оба эти способа буду работать одинаково.

ua = fake_useragent.UserAgent() or ua = UserAgent() во втором примере используем укороченную запись.

from fake_useragent import UserAgent
import requests

url = 'http://httpbin.org/user-agent'

for x in range(10):
  ua = UserAgent()
  fake_ua = {'user-agent': ua.random}
  response = requests.get(url=url, headers=fake_ua)
  print(response.text)

>>>{"user-agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;
chromeframe/13.0.782.215)"}
...

Давайте разберкем этот код.
1. Цикл на 10 итераций;
2. ua = UserAgent() создает экземпляр класса UserAgent;
3. fake_ua = {'user-agent': ua.random} создаем переменную, в которой будет корректный user-agent, используя метод
.random(обращаю ваше внимание на то, что это метод библиотеки UserAgent, не путайте с библиотекой random).
Этот метод гарантирует, что user-agent будет уникален при каждом запросе:
4. Создаем экземпляр класса Requests с методом .get() и передаем в атрибут ссылку и headers с переменной, в которй хранится
корректный user-agent;

p.s.такой способ подмены user-agent мне больше по душе, и в своих парсерах я использую именно его. Бывает, когда необходимо
написать совсем простой код на несколько десятков запросов, я просто копирую user-agent из браузера.

Прокси proxies = 

Библиотека Requests позволяет отправлять запросы, скрывая свой настоящий IP-адрес. Зачем его скрывать?
Например, потому что очень часто за высокочастотные хапросы можно получить бан.

import requests

url='http://httpbin.org/ip'
proxy = {
  'http': 'http://103.177.45.3:80',
  'https': 'http://103.177.45.3:80',
}

response = requests.get(url=url, proxies=proxy)
print(response.json())

_______________________________________________
#Для socks4

proxy_socks4 = {
  'http': 'socks4://103.177.45.3:80',
  'https': 'socks4://1037.177.45.3:80'
}
_______________________________________________
#Для socks5

proxy_socks5 = {
  'http': 'socks5://103.177.45.3:80',
  'https': 'socks5://1037.177.45.3:80'
}
_______________________________________________
#Для всех, с авторизацией

proxy_all_auth = {
  'http': 'socks5://login:password@103.177.45.3:80',
  'https': 'socks5://login:password@1037.177.45.3:80'
}

При запросе через прокси необходимо использовать схемы (http and https). Если сайт, на который вы отправляете запрос, имеет схему 
http, то в словаре proxy необходимо указать именно ее. Если https, указать ее. Метод .get() может сам определить и выбрать 
необходимую схему, поэтому в словаре proxy лучше всего указывать 2 схемы сразу.

Используйте hidemy.name (https://hidemy.name/ru/proxy-list/ - фигня сайт не работает), чтобы скачать 10 шт бесплатных
прокси. И сохраните их в файл с именем proxy.txt

Предположим, у нас есть файл, в котором большое количество прокси, и мы бы хотели использовать именно их.  Это не проблема.

(не рекомендуется брать прокси из этого списка, т.к. они могут уже не работать. Да и прокси на сайте тоже могут не
работать, из собранных 9к прокси работали от силы штук 50. Сайты с бесплатными прокси использовать опасно по двум причинам.
1) Они могут умереть в любую минуту.
2) Они могут быть уже в бане на том сайте, который вы собираетесь парсить.

Вот ссылка - https://github.com/nefelsay/stepik_parsing/blob/main/proxy.py на скрипт который собирает прокси с сайт.

Если потребуются живучие прокси, то рекомендую покупать тут - пишет автор не iDONi - https://proxy6.net/ru/

68.185.57.66:80
68.188.59.198:80
77.73.241.154:80
23.227.38.6:80
203.28.8.133:80
203.32.121.29:80
203.30.190.238:80
141.101.123.224:80
141.101.123.221:80
172.67.3.63:80

Скопируйте этот код и запустите его у себя в терминале, чтобы посмотреть, как он работает.

from random import choice
import requests

url='htt[://httpbin.org/ip'

with open('proxy.txt') as file:
  proxy_file = file.read().split('\n')
  for _ in range(1000):
    try:
      ip = choice(proxy_file).strip()
      proxy = {
        'http':f'http://{ip}'
        'http':f'https://{ip}'
      }
      response = requests.get(url=url, proxies=proxy)
      print(response.json(), 'Success connection')
    except Exception as _ex:
      continue

>>>{'origin': 'httpbin.org, 47.74.152.29'} Success connection
...

Этот код делает запрос к http://httpbin.org/ip и в ответ получает тот ip адрес, который был отправлен в запросе. Этот код немного 
сложнее, чем первый пример.

1) Открыли файл с прокси и считали его, записали список в переменную proxy_file;
2) Начали цикл в 1000 итераций, на каждой выбираем случайным образом 1 шт прокси при помощи модуля random.choice();
3) В словаре proxy формируем прокси со схемой одновременно. Прокси должен быть вида http://203.24.108.168:80;
4) Делаем запрос к сайту из url отправляя словарь с прокси proxies=proxy где хранится всего 2 элемента, на каждой итерации новый
ip;
5) Распечатываем результат в формате .json();
6) Так же обернули все это в try/except чтобы скрипт не падал timeout.

Таймуат timeput = 
Когда мы делаем запрос к сайту, наш парсер должен дождаться ответа от сервера, прежде чем двигаться дальше.
Если ваши запросы используют прокси, то время ожидания каждого запроса может существенно возрасти. По умолчанию requests
будет ждать ответа неопределенное время, поэтому вы почти всегда должны указывать продолжительность тайм-аута.

Напишите код, чтобы убедиться, что время ожидания запроса через несуществующий прокси будет значительным.

import requests
import time

url = 'http://httpbin.org/get'

proxies = {
  'http': 'http://200.12.55.98:80',
  'https': 'http://200.12.55.90:80'
}

start = time.perf_counter()
try:
  requests.get(url=url, proxies=proxies)
except Exception as _ex:
  print(time.perf_counter() - start)

>>>21.056841100013116

Время ожидания ответа на запрос - 21 секунда. Чтобы сократить время этого ожидания до 1 секунды, напишем то же код, 
только с атрибутом timeout=1. А если timeout = None, будем ждать ответ вечно.

import requests
import time

url = 'http://httpbin.org/get'

proxies = {
  'http': 'http://200.12.55.90:80',
  'https': 'http://200.12.55.90:80'
}

start = time.perf_counter()
try:
  
