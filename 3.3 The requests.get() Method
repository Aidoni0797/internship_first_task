3.3 The requests.get() Method

requests.get()

Запросы к серверу можно направлять с самыми разными целями. Но мы будем делать акцент в основном на .get() запросы.
Запросы .get() обычно предназначены только для чтения данных без их изменения. С помощью этого метода мы научимся получать HTML-код страницы, скачивать изображения,
музыку и видео.

Чтобы выполнить .get() запрос, используется команда requests.get()

Это такой же тип запроса, какой направляет на сервер ваш браузер, чтобы открыть вам интернет-страницу. Единственное отличие заключается в том, что библиотека Requests
не может выполнить рендеринг кода HTML, поэтому вы получите просто текст кода HTML. Но пролверьте, там есть все, что нам нужно.

Сайт https://httpbin.org/ создан специально для веб-разработчиков, чтобы они могли тестировать свой код, отправляя свои запросы. Этот сайт возвращает информацию по конкретному 
запросу. Его мы будем использовать весь блок изучения библиотеки requests.

import requests

response = requests.get(url='http://httpbin.org/')

print(type(response))

>>><class 'requests.models.Response'>

Давайте вместе выполним это запрос и посмотрим type() возвращаемого объекта. Как видим, мы получаем объект класса Response, который является экземпляром класса requests.

Надеюсь, от последней фразы вы не запутались. Попробую объяснить проще. В последнем примере переменная response стала экземпляром класса requests и в нее передали
атрибут = url

Мы можем передавать этому классу и другие атрибуты, поднобнее о которых вы узнаете далее.

url - передает ссылку - цель, куда будет отправлен запрос. (Обязатлеьный)
params - словарь или байты, которые будут отправлены в строке запроса. (Необязательный)
headers - словарь HTTP-заголовков, отправляемых с запросом. (Необязательный)
cookies - объект Dict or CookieJar для отправки с запросом. (Необязательный)
auth - AuthObject для включения базовой аутентификации HTTP. (Необязательный)
timeout - Число с плавающей запятой, описывающее тайм-аут запроса. (Необязательный)
allow-redirects - логическое значение. Установите значение True, если разрешено перенаправление. (Необязательный)
proxies - Протокол соспоставления словаря с URL-адресом прокси. (Необязательный)
stream  - Удерживает соединение открытым, пока не получен весь Response.content (Необязательный)

Заголовки headers = 

Заголовки отправляются вместе с запросом и возврщаются с ответом. Они нужны, чтобы клиент и сервер понимали, как интерпритировать данные, отправляемые и
получаемые в запросе и ответе.

При написании парсеров рекомендуется отправлять свои заголовки вместе с .get() запросом, потому что сервер понимает, что делать с нашим запросом, именно исходя из тех
заголовков, которые мы отправили. Иногда необходимо маскироваться под браузер, подменять cookie, user-agent, headers, ip и др.

Вот пример отправляемого нашим скриптом заголовка.

import requests

response = resuets.get(url='http://httpbin.org/user-agent')
print(response.text)

>>>{"user-agent": "python-requests/2.27.1"}

Если сайт простой и программистов не заботят такие мелочи, то мы тоже не переживать и отправлять такие запросы. Но чаще всего это не так.

user-agent:Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTMK, like Gecko)
Chrome/99.0.4844.84 Safari/537.36

Для того, чтобы замаскировать свой запрос под запрос браузера, будем использовать .get() запрос с именованным атрибутом и передадим в него словарь headers.

import requests

headers = {
'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)
Chrome/99.0.4844.84 Safari/537.36',
}

response = requests.get(url='http://httpbin.org/user-agent', headers=headers)
print(response.text)

>>>{"user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)
Chrome/99.0.4844.84 safari/537.36"}

В ответе на ваш запрос мы увидим переданный нами user-agent. Поэкспериментируйте самостоятельно, измените содержания словаря headers и посмотрите, что изменится.

Загрузите файт с user-agent и поместите его в корневой каталог с вашим проектом. Скачять файл - https://drive.google.com/file/d/1mIG_570jp_NSlPgeyCF2xOOZLjP2V82w/view

import requests
from ramdom import choice

url = 'http://httpbin.org/user-agent'

while line := open ('user-agent.txt').read().split('\n'):
  user_agent = {'user-agent': choice(line)}
  response = requests.get(url=url, header=user_agent)
  print(response.text)

>>>{"user-agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)
Chrome/50.0.2661.94 Safari/537.36"}
.....
Этот код последовательно представляет user-agent из файла и делает запрос на наш url. Обратите внимание, что в этом
примере использовался модуль random и его метод choice для случайного выбора user_agent из файла.

Когда вы будете пистаь свой первый парсер, вы можете использовать первый пример, т.к. он намного проще и нужен чаще всего.
Второй пример более сложен, такое необходимо для высокочастотных апрсеров и в тех случаях, когда сервер банит за спам-запросы.
В таком случае можно использовать задержки time.sleep(10).


Библиотека fake_useragent

Библиотека fake_useragent создана, чтобы облегчить нам жизнь. Она делает то же самое, что и код из прошлого степа, только еще проще.

Установка библиотеки
pip install fake-useragent
or
pip3 install fake-useragent

импорт

from fake_useragent import UserAgent
or
import fake_useragent

Экземпляр класса создается двумя способами. Оба эти способа буду работать одинаково.

ua = fake_useragent.UserAgent() or ua = UserAgent() во втором примере используем укороченную запись.

from fake_useragent import UserAgent
import requests

url = 'http://httpbin.org/user-agent'

for x in range(10):
  ua = UserAgent()
  fake_ua = {'user-agent': ua.random}
  response = requests.get(url=url, headers=fake_ua)
  print(response.text)

>>>{"user-agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;
chromeframe/13.0.782.215)"}
...

Давайте разберкем этот код.
1. Цикл на 10 итераций;
2. ua = UserAgent() создает экземпляр класса UserAgent;
3. fake_ua = {'user-agent': ua.random} создаем переменную, в которой будет корректный user-agent, используя метод
.random(обращаю ваше внимание на то, что это метод библиотеки UserAgent, не путайте с библиотекой random).
Этот метод гарантирует, что user-agent будет уникален при каждом запросе:
4. Создаем экземпляр класса Requests с методом .get() и передаем в атрибут ссылку и headers с переменной, в которй хранится
корректный user-agent;

p.s.такой способ подмены user-agent мне больше по душе, и в своих парсерах я использую именно его. Бывает, когда необходимо
написать совсем простой код на несколько десятков запросов, я просто копирую user-agent из браузера.

Прокси proxies = 

Библиотека Requests позволяет отправлять запросы, скрывая свой настоящий IP-адрес. Зачем его скрывать?
Например, потому что очень часто за высокочастотные хапросы можно получить бан.

import requests

url='http://httpbin.org/ip'
proxy = {
  'http': 'http://103.177.45.3:80',
  'https': 'http://103.177.45.3:80',
}

response = requests.get(url=url, proxies=proxy)
print(response.json())

_______________________________________________
#Для socks4

proxy_socks4 = {
  'http': 'socks4://103.177.45.3:80',
  'https': 'socks4://1037.177.45.3:80'
}
_______________________________________________
#Для socks5

proxy_socks5 = {
  'http': 'socks5://103.177.45.3:80',
  'https': 'socks5://1037.177.45.3:80'
}
_______________________________________________
#Для всех, с авторизацией

proxy_all_auth = {
  'http': 'socks5://login:password@103.177.45.3:80',
  'https': 'socks5://login:password@1037.177.45.3:80'
}

При запросе через прокси необходимо использовать схемы (http and https). Если сайт, на который вы отправляете запрос, имеет схему 
http, то в словаре proxy необходимо указать именно ее. Если https, указать ее.
