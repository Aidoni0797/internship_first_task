8.7 aiohttp

Введение в aiohttp

Установка: pip install aiohttp

Импорт: import aiohttp

Для асинхронных запросов к серверу нам не подойдет стандартная библиотека requests, которой мы 
пользовались ранее. Для этого существует асинхронная библиотека запросов aiohttp.

aiohttp(documentation-https://docs.aiohttp.org/en/stable/) - асинхронный ввод/вывод для http-запросов.
Он был реализован на базе asyncio, использующей модель кооперативной многозадачности. Как мы уже знаем,
в python существует GIL, который не позволяет двум потокам работать одновременно, заставляя весь код
работать последовательно. При написании парсеров можно обойтись без асинхронного кода и писать простые 
парсеры для фриланса или для своих целей. Но пытливый ум программиста заставляет каждого из нас продолжать
учиться и совершенствовать свои навыки. Именно поэтому и был придуман модуль aiohttp для асинхронных
запросов к серверу. Aiohttp изначально создавался для высоконагруженных систем, которые должны
выдерживать огромное количество запросов в секунду, но нам он нужен для ускорения наших парсеров, с чем
данный модуль справляется на отлично.

Aiohttp имеет подробную документацию, в которой можно найти все ответы на свои вопросы, что является
дополнительным аргументом для выбора его в качестве рабочего инструмента.

Aiohttp

Сейчас будет самый простой пример асинхронного запроса. На полноценный парсер это еще не похоже, т.к.
выполняется всего один запрос, но для старта нам подойдет.

Первым делом необходимо создать сессию. Сессия обладает набором параметров по умолчанию, которые
передаются серверу с каждым запросом в рамках этой сессии. Самый важный параметр - это cookie,
которые являются общим для всех запросов сессии.

В коде ниже мы создали сессию при помощи менеджера контекста with. Это нужно для того, чтобы после
выполнения всех запросов aiohttp очистил все ресурсы.

Если у вас не запускается код ниже, то удалите следующую строку.

asyncio.set_event_loop_policy(asyncio.WindowaSelectorEventLoopPolicy())

import aiohttp
import asyncio

async def main():
#---------------------start block 2---------------------

async with aiohttp.ClientSession(trust_env=True) as session:
  async with session.get('https://parsinger.ru/html/index1_page_1.html') as response:
    print(await response.text())
#-------------------end block 2-------------------

#--------------------start block 1 ---------------------------
asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
asyncio.run(main())
#--------------------start block 1 --------------------------

При выполнении этого когда происходит следующее. Интерпретатор, проходя по коду сверху вниз,
инизиализирует нашу корутину async def main() в блоке №2, на этом этапе запуск еще не происходит. 
Когда интерпретатор доходит до блока №1, он применяет политику, по правилам которой будет работать
цикл событий, это происходит в первой строке первого блока, об этом мы говорили в разделе про
event loop. Когда интерпретатор доходит до второй строки блоки №1, где мы передаем нашу корутину
на выполнение в цикл событий asyncio.run(main()), именно в этот момент происходит ее выполнение.

Вся магия асинхронного запроса происходит в блоке №2. Первая строка этого блока создает с помощью
менеджера контекста with асинхронную сессию, которая может быть передана в другую корутину. Но в этом
примере она используется в той же корутине. Во второй строке второго блока мы используем метод
.get(url="") для передачи этой сессии ссылки для асинхронного запроса. В следующих запросах мы 
поговорим о том, как передать в него целый список ссылок и как обработать их асинхронно.

На этом этапе уже можно понять, как мы будем использовать bs4 для извлечения нужных элементов со
странице, но об этом чуть позже.

В третьей строке мы печатаем все содержимое ответа. У вас может появиться справедливый вопрос: а
почему мы написали await в функции print()? Т.к. response работает в сетью, возвращая нам результат
своей работы через устройство ввода/вывода, т.е. через HDD/SSD/M2 и т.д., код вынужден ждать, пока 
произойдет эта операция. Если бы у нас было больше запросов чем один, в этом месте произошло бы
переключение контекста на другой запрос.

Как можно заметить, переменная response ведет себя почти так же, как и в синхронной библиотеке
requests, о чем мы и поговорим в следующем абзаце.

Какое содержимое мы можем получить от переменной response?

1. await response.text() - возвращает содержимое страницы, весь HTML.
2. response.content(без использования await) - возвращает размер содержимого в байтах
<StreamReader 7875 bytes eof>
3. response.status(без использования await) - вохвращает статус код ответа.
4. response.url(без использования await) - возвращает текущий URL на который выполнялся запрос.
5. await response.read() - возвращает содержимое страницы в байтовом виде, на примере тега title это
наглядно видно.

*<title>\x68\xa3\xd1\x87\x6B\x68\x68\xbc\xd1\x81\x81\x87
xdB\xbfd\x6B\x01\x88\xd1\x81\xdB\xhB\xd1\x82\x61\x8c</title>

6. await response.json() - возвращает результат в формате JSON, в том случае, если сервер готов нам его
отдать.

7. response.headers (без использования await) - возвращает заголовок страницы.

8. response.cookies - возвращает объект <class 'http.cookies.SimpleCookie'>, т.к. у нас создается сессия,
мы можем получить куки, которые были выданы сервером, для дальнейшего использования. С кукисами
в этом формате можно работать как со словарем, используя методы .items(), .value(), .get() 
и другие методы словарей.

* Кукисы полученные этим способом имеют вид

Set-Cookie: GPS=1; Domain=youtube.com; expires=Tue, 06-Sep-2022 10:50:55 GMT; HttpOnly; Path=/;
Secure
Set-Cookie: VISITOR-INFO01_LIVE=1v9mQ2FgdTM; Domain=youtube.com; expires=Sun, 05-Mar-2023 10:20:55
GMT; HttpOnly; Path=/; SameSite=none; Secure
Set-Cookie: YSC=UIIiWEGDShE; Domain=youtube.com; HttpOnly; Path=/; SameSite=none; Secure

9. response.charset(без использования await) - возвращает текущую кодиовку данных ответа от сервера.

10. response.reason(без использования await) - возвращает строковое значение "ОК" если статус код меньше
400, вернёт NotFound если статус код больше 400, или оповестить другой ошибкой.

11. response.host(без использования await) - вернёт имя домена, на примере кода выше ответ будет 
parsinger.ru

12. response.request_info(без использования await) вернёт информацию о запросе, на примере кода выше
ответ будет

* RequestInfo(url=URL('https://parsinger.ru/html/index1_page_1.html'), method='GET', headers=
<CIMultiDictProxy('Host':'parsinger.ru', 'Accept':'*/*', 'Accept-Encoding': 'gzip, deflate',
'User-Agent':'Python/3.10 aiohttp/3.8.1')>,
real_url=URL('https://parsinger.ru/html/index1_page_1.html'))

Эти были основные методы и функции для извлечения данных о зпаросе. Более подробно можно изучить в
документации.

Aiohttp часть 2

Во второй части мы поговорим о том, какие дополнительные аргументы принимает session.get().

Если у вас не запускается код ниже, то удалите следующую строку.

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())


import aiohttp
import asyncio

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)
Chrome/105.0.0.0 Safari/537.36'}
url = 'http://httpbin.org/get'
data={'sessiokKey': '9ebbd0b25760557393a43064a92bae539d962103', 'format': 'xml', 'platformId':1}

async def main():
  async with aiohttp.ClientSession(trust_env=True) as session:
    async with session.get(url=url, headers=headers, timeout=1, params=data) as response:
      print(await response.text())

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
asyncio.run(main())

1. session.get(url=https://example.com) - устанавливает ссылку, на которую будет выполнен запрос.
2. session.get(timeout=1) - устанавливает timeout ожидания ответа от сервера, если ответ не пришёл
в указанное время будет получена ошибка asyncio.exceptions.TimeoutError.
3. session.get(headers=headers) - устанавливает заголовки запроса аналогично синхронной библиотеке 
requests.
4. session.get(params=data) - устанавливает словарь с дополнительными параметрами. Параметры при запросе
методом .get() формируют следующую ссылку. Попробуйте запустить этот код у себя и поиграйтесь со словарем
data, чтобы понять смысл передачи параметров.

#такая ссылка будет сформирована с помощью словаря data
http://httpbin.org/get?sessionKey=9ebbd0b25760557393a43064a92bae539d962103&format=xml&platformId=1

Aiohttp работа с proxy

Aiohttp поддерживает работу c HTTP и HTTPS, а вот с SOCKS для обстоят немного сложнее. В этом степе мы 
рассмотрим работу со всеми видами прокси.

Код ниже предемонстрирует ростое использование HTTP-прокси с применением basic authentiocation, которое
используется на большинстве сайтов.

import aiohttp
import asyncio

url = 'http://httpbin.org/ip'

async def main():
  proxy = 'http://127.0.0.1:80'
  async with aiohttp.ClientSession(trust_env=True) as session:
    proxy_auth = aiohttp.BasicAuth('user', 'pass')
    async with session.get(url=url, proxy=proxy, proxy_auth=proxy_auth) as response:
      print(await response.text())

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
asyncio.run(main())

В коде выше добавилась одна новая строка proxy_auth = aiohttp.BasicAuth('user', 'pass'), которая
управляет авторизацией к прокси-серверу. Также обратите внимание, что мы передаем прокси формата
'http://127.0.0.1:80', аналогично тому, как мы писали в библиотеке requests.

Если ваши прокси не требует авторизации по логину и паролю, достаточно убрать переменную proxy_auth.

Код ниже выполнит запрос через прокси. Если у вас есть рабочий прокси, то можете запустить этот код с ним.
Если рабочей прокси у вас нет, то качественные прокси продаются тут. (https://proxy6.net/ru/)

import aiohttp
import asyncio

url = 'http://httpbin.org/ip'

async def main():
  proxy = 'http://127.0.0.1:80'
  async with aiohttp.ClientSession() as session:
    async with session.get(url=url, proxy=proxy) as response:
      print(await response.text())

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
asyncio.run(main())

В коде выше добавилась одна новая строка proxy_auth = aiohttp.BasicAuth('user', 'pass'),
которая управляет авторизацией к прокси-серверу. Также обратите внимание, что мы передаем
прокси формата 'http://127.0.0.1:80/', аналогично тому, как мы писали в библиотеке requests.

Если ваши прокси не требует авторизации по логину и паролю, достаточно убрать переменную
proxy_auth.

Код ниже выполнит запрос через прокси. Если у вас есть рабочий прокси, то можете запустить
этот код с ним. Если рабочей прокси у вас нет, то качественные прокси продаются тут (https://proxy6.net/ru/).

import aiohttp
import asyncio

url = 'http://httpbin.org/ip'

async def main():
  proxy='http://127.0.0.1:80'
  async with aiohttp.ClientSession() as session:
    async with session.get(url=url, proxy=proxy) as response:
      print(await response.text())

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
asyncio.run(main())

Для запуска следующего кода нам понадобится файлик с прокси. Скачать файл с паблик прокси можно тут,
(iDONi скачала и загрузила в репозоторий) вряд ли они окажутся живыми, но для понимания
кода нам это и не требуется. А если ваш внутренний перфекционист требует свежие прокси,
то в этом степе(iDONi ссылку не поставить потому что этот курс платный, а iDONi только на еду хватает)
 есть рабочий парсер прокси с сайта hidemy.name(ссылка на сайт не рабочий, Бог знает почему так),
можете собрать сами или купите 100% рабочие по ссылке выше.

import aiohttp
import asyncio
import aiofiles

async def main():
  async with aiofiles.open('proxy_list.txt', mode='r') as f:
    for prx await f.readlines():
      proxy = f'http://{prx}'
      url='http://httpbin.org/ip'
      try:
        async with aiohttp.ClientSession() as session:
          async with session.get(url=url, proxy=proxy, timeout=1) as response:
            if response.ok:
              print(f"good proxy, sestus_code - {response.status} - ", prx, end='')
            else:
              print(f"bad proxy, status_code - {response.status} - ", prx, end='')
      except Exception as _ex:
        continue

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
asyncio.run(main())

Результат:
  good proxy, status_code - 200 - 91.226.97.77:80
  bad proxy, status_code -409- 91.226.97.116:80
  good proxy, status_code -200- 203.28.9.92:80
  bad proxy, status_code -409- 203.24.108.64:80
  ...
  bad proxy, status_code -409- 185.238.228.238:80
  bad proxy, status_code -403- 172.67.165.107:80
  good proxy, status_code -200- 203.24.109.182:80
  bad proxy, status_code -409- 45.12.31.95:80

Про aiofiles мы будем говорить в следующем разделе курса.

Если коротко, то этот код пингует сайт https://httpbin.org/ip через предоставленные нами
прокси и делает это в асинхронном стиле. Такой код будет работать быстрее синхронного. 
В этом коде установлен timeout=1, в это значит, что синхронный код ждал бы одну секунду, 
после чего возвращал бы ответ. В нашем списке имеется -9200 тш прокси, итого, мы
потратим 9200/60 = 153 минуты на проверку всех прокси. В асинхронном стиле проверка всех
прокси заняла - 3500 сек, что почти в 2.6 раза быстрее. Ключевая разница двух подходов в том,
что синхорнный код бездействует, пока ждет ответа от сервера, т.е. 60% времени ваш синхронный код
ничего не делает. Асинхронный код во время ожидания ответа новый запрос, при таком подходе
время бездействия крайне мало.

В коде выше мы придаём вид.

http://203.30.190.95:80
http://185.162.231.55:80
http://203.30.190.152:80
http://203.28.9.1:80

Если мы хотим использовать прокси с авторизацией по логину и паролю, то вид у прокси должен быть
следующий.

http://login:password@203.28.8.151:80
http://login:password@162.247.243.185:80
http://login:password@172.67.186.212:80
http://login:password@58.204.190.234:80

или использовать BasicAuth, подробнее в документации(https://docs.aiohttp.org/en/stable/client_advanced.html#proxy-support).
proxy = 'http://23.227.38.102:80'
proxy_auth = aiohttp.BasicAuth('your_user', 'your_password')
async with session.get(url, proxy=proxy, proxy_auth=proxy_auth) as response:
  return BeautifulSoup(await response.content, 'html.parser')


Aiohttp работа с SOCKS

В aiohhtp работа с SOCKS отличается от работы с простыми HTTP/HTTPS прокси, и в этом 
степе мы подробно об этом поговорим.

Для работы с SOCKS4 and SOCKS5 нам необходимо использовать класс ProxyConnector из модуля
aiohttp_socks, для этого его нужно импортировать, заодно импортируем ProxyType и ChainProxyConnector,
они немного сложнее в работе, и для написания парсера с множеством прокси они подходят 
на совсем хорошо, но знать про них нужно.

from aiohhtp_socks import ProxyType, ProxyConnector, ChainProxyConnector

ProxyConnector

ProxyConnector - это самый простой способ передать socks4 or socks5 в асинхронную 
сессию, для этого нужно передать прокси как продемонстрировано в коде ниже.

async def main(url):
  connector = ProxyConnector.from_url('socks5://user:password@127.0.0.1:1080')

  async with aiohttp.ClientSession(connector=connector, trust_env=True) as session:
    async with session.get(url) as response:
      return await response.text()

Мы можем использовать конструктор и настроить параметры нашего прокси, такой вариант 
удобен, если вы используете один прокси для ваших запросов. Как правилло, для написания 
парсеров использовать конструктор не обязательно.

connector = ProxyCOnnector(
  proxy_type=ProxyType.SOCKS5,
  host='127.0.0.1',
  port=1080,
  username='user',
  password='password',
  rdns=True
)

Самая распространенная ситуация - когда у вас есть список рабочих прокси, вы хотите
поочередно передать каждый прокси в сессию, тем самым увеличивая время жизни вашего 
парсера. Для этого нам понадобится код с прошлого степа, который мы немног изменили.

Если у вас нет SOCKS, то вы можете воспользоваться моим измененным парсером, он собирает
прокси с сайта https://hidemy.name/ru/proxy-list/, теперь он собирает только SOCKS4 and SOCKS5
как всегда на этом сайте из 108 собранных SOCKS оказались рабочими 10 шт.

Сам парсер лежит тут (https://github.com/nefelsay/stepik_parsing/blob/main/parse_proxy_socks.py)

Для этого запуска этого кода вам потребуется файл proxy_list_SOCKS.txt, который создается
после работы моего парсера. Если у вас есть свои SOCKS, то поместите их в этот файл.
Обратите внимание, что мы передаем в сессию только connector=connector, в котором
на каждой итерации цикла будет хранится новый SOCKS.

Этот код создает на каждой итерации новую ClientSession(), в которую мы передаем параметры
ProxyConnector, в котором на каждой итерации мы передаем новый прокси. Пересоздавать на каждой
итерации новую сессию считается не очень хорошей идеей, но для небольших парсеров впоне сгодится.
Сильная сторона такого подхода в том, что при плохом прокси в списке код продолжит работать
со средующим, и так до тех пор, пока прокси не закончатся. Как делать повторные запросы
через прокси, которые были забанены, мы поговорим в следующем разделе курса.

import aiohttp
import asyncio
import aiofiles
from aiohttp_socks import ProxyConnector

async def main():
  async with aiofiles.open('proxy_list_SOCKS.txt', mode='r') as f:
    for prx in await f.readlines():
      url='http://httpbin.org/ip'
      connector = ProxyConnector.from_url(f'socks4://{prx}')
      try:
        async with aiohttp.ClientSession(connector=connector, timeout=.5) as session:
          async with sesison.get(url=url, timeout=1) as response:
            if response.status:
              print(f'good proxy, status_code - {response.status} - ', prx, end='')
      except Exception as _ex:
        continue

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
asyncio.run(main())


ChainProxyConnector

ChainProxyConnector полезен тем, что мы можем использовать сразу большой список прокси.
Ваш список прокси может быть любой длины и вы можете поместить его в память программы, 
записав его в переменную. Но это не совсем хорошая практика, потому что при большом
количестве прокси ваш код будет выглядеть ужасно. from_urls([list]) получает на вход
список, котрый может быть извлечен напрямую из файла. В следующих примерах мы рассмотрим
как это сделать.

#Передаём список с прокси напрямую в from_urls()
connector = ChainProxyConnector.from_urls([
  'socks5://user:password@127.0.0.1:1080',
  'socks4://127.0.0.1:1081',
  'http://user:password@127.0.0.1:3128',
])

async with aiohttp.ClienSession(connector=connector) as session:
  asyn with session.get(url) as response:
    return await response.text()

У такого подхода есть одна очень важная особенность, которая портит нам всю малину.
Вам нужно указывать схему, по которой работают ваши прокси socks4://, socks5:// или
http://, socks4 не будут рабоать по схеме socks5. По этой причине нужно заранее
знать, какие вы используете прокси. Для парсинга хорошо подходят socks4 и socks5.

В этом коде мы открываем файл SOCKS5.txt, который содержит прокси вида.

Читаем построчно прокси из файла, обрабатывая каждую и добавляя к ней схему, по
которой она рабоатет, формируем список proxy для передачи в ChainProxyConnector.

import aiohttp
import asyncio
import aiofiles
from aiohttp_socks import ChaimnProxyConnector

async def main():
  url = 'https://httpbin.org/ip'
  proxy=[]
  async with aiofiles.open('SOCKS5.txt', mode='r') as socks4:
    for s4 in await socks4.readlines():
      proxy.append(f"socks5://{s4}')

  connector = chainProxyConnector.from_urls(proxy)
  async with aiohttp.ClientSession(connector=connector)as session:
    async with session.get(url=url) as response:
      if response.status:
        print(f'good proxy, status_code - {response.status} - ', s4, end='\n')
      elif response.status >=400:
        print(f'bad proxy, status_code - {response.status} - ', s4, end='\n')

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
asyncio.run(main())

Ниже тот же самый код, только теперь он берет прокси непосредственно из списка 
proxy = []. Такой код будет работать, если все прокси в списке живые, этот код упадет
при первом нерабочем прокси и вы получите ошибку =>

[Errno 11001] Could not connect to proxy 194.28.210.392:9867 [getaddrinfo failed]

import aiohttp
import asyncio
from aiohttp_socks import ChainProxyConnector

async def main():
  url = 'http://httpbin.org/ip'
  proxy = [
    'socks5://D2Frs6:75JjrW@194.28.210.39:9867',
    'socks5://D2Frs6:75JjrW@194.28.209.68:9925'
  ]
  connector = ChainProxyConnector.from_urls(proxy)
  try:
    async with aiohttp.ClientSession(connector=connector, timeput=5, trust_env=True) as session:
      async with session.get(url=url, timeout=1) as response:
        if response.status:
          print(f'good proxy, status_code - {response.status} - ', end='')
        elif response.status >= 400:
          print(f'bad proxy, status_code - {response.status} - ', end='')
  except Exception as _ex:
    print(_ex)

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
asyncio.run(main())

Proxy
