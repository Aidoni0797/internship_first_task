4.5 Pagination

Пагинация

Пагинация (от слова page - "страница") повсюду, нам приходится работать с ней почти всегда при написании парсера. В этом блоке мы изучим пагинацию, научимся проходиться
по ней в цикле и извлекать информацию на каждой странице.

В нашем тренажере - https://parsinger.ru/html/index1_page_1.html - она тоже есть. На скриншоте выделено 2 облатси. В верхней области мы видим ссылку, которая заканчивается
числом, как правило, число = номер страницы. Мы можем изменить ссылку и делать запросы на каждой итерации. Во второй области выделены кнопки пагинации.

В нашем тренажере простая пагинация, она имеет всего 4 страницы, но это достаточно для понимания.

Итак, для начала нам необходимо получить общее количество страниц. В этом нам помогает пагинация. Давайте посмотрим на HTML код пагинации.

Мы видим, что здесь она представляет собой блок <div class='pagen'>, в котором 4 тега  <a>. Мы уже умеем пользоваться методами .find() and .find_all(), давайте применим их 
и соберем ссылки из пагинации, также нам понадобится значение последнего элемента.

from bs4 import BeautifulSoup
impot requests

url = 'http://parsinger.ru/html/index1_page_3.html'
response = requests.get(url=url)
response.encoding='utf-8'
soup=BeautifulSoup(response.text, 'lxml')
pagen = soup.find('div', class_='pagen').find_all('a')
print(pagen)

>>>[<a href='index1_page_1.html'>1</a>, <a href='index1_page_2.html'>2</a>, <a href='index1_page_3.html'>3</a>, <a href='index1_page_4.html'>4</a>]

Мы получили список тегов, но нам ведь нужно извлечь ссылку и текст. Применим list comprehension, чтобы сделать это удобнее.

pagen = [link['href'] for link in soup.find('div', class_='pagen'0\).find_all('a')]

>>> ['index1_page_1.html', 'index1_page_2.html', 'index1_page_3.html', 'index1_page_4.html']

Если вы не понимаете как работает list comprehension: пример ниже сделает то же самое, но в обычном цикле, результат будет идентичный. А вообще советую почитать
про него дополнительно, для программиста на python это очень нужный и полезный инструмент.

list_link = []
for link in pagen:
  list_link.append(link['href'])

print(list_link)

>>> ['index1_page_1.html, 'index1_page_2.html', 'index1_page_3.html', 'index1+page_4.html']

Обратите внимание на то, как мы получаем значение атрибута href='', подобным образом мы можем извлекать ссылку из тегов <a>. Такой подход применим и к тегу <img>,
мы сможем извлечь src='', где хранится ссылка на изображение. Картинки мы будем парсить в следующих уроках.

Итак, что мы имеем? А имеем мы список, в котором хранятся 4 имени файла, и нужно превратить их в ссылки. Вероятно, вы уже догадались как это сделать. Если вы 
предположили, что это будет f'' - строка, то вы совершенно правы.

Давайте проанализируем, как формируется ссылка на пагинацию, и сформируем схему, которая поможет генерировать корректные ссылки.

За схемой далеко ходить не нужно, а адресной строке мы можем ее увидеть.

stepik-parsing.ru/html/index1_page_3.html

Создадим переменную shema и сохраним в нее первую часть ссылки. И в цикле на каждой итерации мы будем склеивать обе части, чтобы получить корерктную ссылку.

from bs4 import BeautifulSoup
import requests

url = 'http://parsinger.ru/html/index1_page_3.html'
response = requests.get(url=url)
response.encoding='utf-8'
soup = BeautifulSoup(response.text, 'lxml')
pagen=soup.find('div', class_='pagen').find_all('a')

list_link = []
shema='http://parsinger.ru/html'
for link in pagen:
  list_link.append(f"{shema}{link['href']}")

print(list_link)

>>>['http://parsinger.ru/html/index1_page_1.html','http://parsinger.ru/html/index1_page_2.html', 'http://parsinger.ru/html/index1_page_3.html', 'http://parsinger.ru/html/index1_page_4.html']

Отлично, всё получилось.

То же самое, но с применением list comprehension:

from bs4 import BeautifulSoup
import requests

url = 'http://parsinger.ru/html/index1_page_3.html'
response = requests.get(url=url)
response.encoding='utf-8'
soup=BeautifulSOup(response.text, 'lxml')
shema='http://parsinger.ru/html'
pagen=[f"{shema}{link['href']}" for link in soup.find('div', class_='pagen').find_all('a')]

print(pagen)

>>> ['http://parsinger.ru/html/index1_page_1.html','http://parsinger.ru/html/index1_page_2.html','http://parsinger.ru/html/index1_page_3.html','http://parsinger.ru/html/index1_page_4.html']


Пагинация часть - 2

Часто бывает, что мы не можем получить все ссылки из пагинации и нам приходится генерировать их самостоятельно. Звучит странно, но вы сейчас поймете, о чем я.
Для примера посмотрим на ссылку маркета www.wildberries.ru

wildberries.ru/catalog/elektronika/smart-chasy?sort=popular&page=8

Самые догадливые, надеюсь, уже поняли, что мы будем делать дальше. Правильно, генерировать ссылки. Делается это при помощи f'' строки.

link = []
for i in range(1, 101):
  link.append(f'https://www.wildberries.ru/catalog/elektronika/smart-chasy?sort=popular&page={i}')
print(link)

>>> [
  'https://www.wildberries.ru/catalog/elektronika/smart-chasy?sort=popular&page=1',
  'https://www.wildberries.ru/catalog/elektronika/smart-chasy?sort=popular&page=2',
  ............
  'https://www.wildberries.ru/catalog/elektronika/smart-chasy?sort=popular&page=100']

Подобным образом мы получим сразу 100 готовых для парсинга ссылок. Загвоздка заключается в том, что мы заранее не знаем, сколько на странице ссылок в пагинации.
А вдруг завтра количество страниц может быть увеличено или уменьшено, и наш парсер либо упадет с ошибкой, либо не дотянется до новых страниц?

Поэтому нам нужно извлечь последнее значение из пагинации перед генерацией ссылок.

За основу примера возьмём на тренажер - https://parsinger.ru/html/index1_page_1.html

from bs4 import BeautifulSoup
import requests

url = 'http://parsinger.ruhtml/index1_page_3.html'
response = requests.get(url=url)
response.encoding = 'utf-8'
soup = BeautifulSoup(response.text, 'lxml')
shema = 'http://parsinger.ru/html/'
pagen = [link.text for link in soup.find('div', class_='pagen').find_all('a')[-1]]

print(pagen)

>>>4

Мы применили индексацию [-1], чтобы получить последний элемент списка, в котором хранился весь список значений пагинации.

Выполните код выше у себя в терминале, посмотрите на результат, без метода .text и без индекса.
Попытайтесь понять, почему мы используем этот подход.
]

Задание 1
1. Откройте сайт - https://parsinger.ru/html/index3_page_1.html
2. Извлеките название товара с каждой страницы (всего 4х страниц)
3. Данные с каждой страницы должны хранится в списке.
4. По итогу работы должны получится 4 списка которые хранятся в списке (список списков)
5. Отправьте получившийся список списков в поле ответа.
6. Метод strip() использовать не нужно.

Пример ожидаемого списка
[['name1','name2','...','name_N'],['name1','name2','...','name_N'],['name1','name2','...','name_N'],['name1','name2','...','name_N']]

Решение
import requests
from bs4 import BeautifulSoup
import sys
sys.stdout.reconfigure(encoding='utf-8')

base_url = "https://parsinger.ru/html/index3_page_{}.html"
result = []

for page in range(1, 5):
    url = base_url.format(page)
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    items = soup.find_all('a', class_='name_item')
    titles = [item.text for item in items]
    result.append(titles)

print(result)

Бог знает, правильно или нет (ChatGPT молодец)

Задание2
1. Открываем сайт - https://parsinger.ru/html/index3_page_4.html
2. Проходимся по всем страницам в категории мыши (всего 4 страницы)
3. На каждой странице посещаем каждую карточку с товаром (всего 32 товаров)
4. В каждой карточке извлекаем при помощи bs4 артикул <p class="article">Артикул:80244813</p>
5. Складываем (плюсуем) все собранные значения
6. Вставляем получившийся результат в поле ответа

Решение:

import re
import requests
from bs4 import BeautifulSoup
import sys
sys.stdout.reconfigure(encoding='utf-8')

total = 0
base_url = "https://parsinger.ru/html/index3_page_{}.html"

for page in range(1, 5):
    response = requests.get(base_url.format(page))
    soup = BeautifulSoup(response.text, 'lxml')
    links = [f"https://parsinger.ru/html/{a['href']}" for a in soup.find_all('a', class_='name_item')]

    for link in links:
        product_resp = requests.get(link)
        product_soup = BeautifulSoup(product_resp.text, 'lxml')
        article_text = product_soup.find('p', class_='article').text
        article_number = int(re.search(r'\d+', article_text).group())
        total += article_number

print("Сумма артикулов:", total)
