8.8 Cooking Asynchronous Soup

Приготовление асинхронного супа

Самая долгожданная частьасинхронного модуля. В этом разделе мы наконец-то напишем свой 
первый асинхронный парсер, который ускорит сбор информации в десятки раз.

Для начала напишем самый просто парсер, который собирает с одной страницы нашего сайта тренажера
всего лишь названия и цены. Большую часть кода вы уже видели, и в этом примере все будет
вам очень знакомо.

Этот пример важен для понимания того, как мы будем строить дальнейшее обучение.

import aiohttp
import asyncio
from bs4 import BeautifulSoup

async def main():
  url = 'https://parsinger.ru/html/index1_page_1.html'
  async with aiohttp.ClientSession() as session:
    async with session.get(url=url, timeout=1) as response:
      soup = BeautifulSoup(await response.text(), 'lxml')
      name = soup.fins_all('a', class_='name_item')
      price = soup.find_all('p', class_='price')
      for n, p in zip(name, price):
        print(n.text, p.text)

asyncio.set_event_loop_policy(asyncio.WindwosSelectorEventLoopPolicy())
asyncio.run(main())

Результат:
  Jet Kid Start blue Умные детские часы 2310 руб
  Band 6 FOREST GREEN FARA-B19 HUAWEI 5480 руб
  Умные часы GT 3 MIL-B19S BLACK HUAWEI 21810 руб
  Умные часы GT 3 MIL-B19V BLACK HUAWEI 21810 руб
  GT RUNNER-B19S BLACK HUAWEI 27770 руб
  GT RUNNER-B19A GREY HUAWEI 27770 руб
  Умные часы GT 3 MIL-B19 GOLD HUAWEI 24230 руб
  Умные часы WATCH 3 GAILEO-L11 STEEL 32600 руб

Все очень просто и понятно, к этому этапу курса вы уже умеете работать с супом, и в
асинхронном коду это почти не отличается от синхронного стиля, за некоторыми исключениями.
К примеру, появилось ключевое слово await, которое распологается перед ответом переменной
await response.text(), в этом месте происходит отправка запроса и получение ответа
отсервера, поэтому нам нужно написать ключевое слово await, чтобы дать понять нашему 
циклу событий, где нам нужно переключаться, пока мы ожидаем ответ от сервера.

soup = BeautifulSoup(await response.text(), 'lxml')

Следующий пример будет немного сложнее, т.к. мы будем получать с нашего сайта (https://parsinger.ru/html/index1_page_1.html)
транажера инфу с карточек, которых там 160 шт, цену и наименование товара.

Запустите код у себя в терминале и попробуйте понять, что мы тут написали, а если все равно
не понятно, то ниже будет полное описание.

import asyncio
import aiohttp

#----------------start block1-----------------
links = [f"https://parsinger.ru/html/index-{index}_page_{page}.html" for index in range(1,5)
for pagen in range(1,5)]
categoiry=['watch', 'mobile', 'mouse', 'hdd', 'headphones']
urls = [f'https://parsinger.ru/html/{cat}/{i}/{i}_{x}.html' for cat, i in zip(category,
range(1, len(category)+1)) for
x in range(1, 33)]
#------------end block 1 --------------------

#----------------start block 2 ---------------
async def main(url):
  async with aiohttp.ClientSession() as session:
    async with session.get(url) as resp:
      soup = BeautifulSoup)await rest.text(), 'lxml')
      price = soup.find('span', id = 'priice').text
      name = soup.find('p', id='p_header').text
      print(resp.url, price, name)
#-------------------end block 2------------------

#------------start block 3----------------
if _name_=='_main_':
  task = [main(link) for link in urls]
  asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
  asyncio.run(asyncio.wait(task))
#--------------end block 3------------------

Результат:
https://parsinger.ru/html/mobile/2/2_17.html 1720 руб Мобильный телефон NOKIA 106 DS TA-1114 серый
https://parsinger.ru/html/mobile/2/2_22.html 2620 руб Мобильный телефон Alcatel 1066D белый
...
https://parsinger.ru/html/watch/1/1_1.html 2310 руб Jet Kid Start blue Умные детские часы
https://parsinger.ru/html/hdd/4/4_26.html 7670 руб Жесткий диск 3.5 2 Tb 7200 rpmrpm 256 MbMb cache Wastern
Digital WD20EZBX SATA III 6 Gb/s

1. Блок кода №1.
  - В этом блоке мы проанализировали наш сайт https://parsinger.ru/html/index1_page_1.html
и поняли, что проще всего будет подготовить все ссылки заранее и передать их все сразу
в цикл событий. Это самый простой,Э но не совсем надежный способ, потому что этот код
может собрать только то, что мы сгенерировали, а если на сайте появится дополнительная
категория или страница пагинации, то код до них не доберется. Но, тем не менее, этот подход
имеет право на существование из-за своей простоты. Вот так выглядит сгеренированный список 
ссылок.

https://parsinger.ru/html/watch/1/1_1.html
https://parsinger.ru/html/watch/1/1_2.html
https://parsinger.ru/html/watch/1/1_3.html
...
https://parsinger.ru/html/headphones/5/5_30.html
https://parsinger.ru/html/headphones/5/5_31.html
https://parsinger.ru/html/headphones/5/5_32.html

2. Блок кода №2.

- Здесь все очень знакомо и просто, вы видели его в предыдущем разделе, посвященном
aiohttp, и по предыдущему примеру кода все должно быть понятно. Вы уже неоднократно 
работали с BeautifulSoup, тут вопросов быть не должно.

3. Блок кода №3.

- А вот тут начинается самой интересное. task = [main(link) for link in urls], т.к.
результатов выполнения асинхронной функции является корутина, в переменную task попадают
объекты корутины. Чтобы в этом убедиться, измените этот блок кода таким образом,
каждый объект списка task будет иметь класс корутины <class 'coroutine'>. Другими
словами, все, что происходит в асинхронной функции main(), переобразуется в специальный
объект, который может работать внутри event loop. Вы же помните, что цикл событий
умеет работать тольлко с корутинами, именно по этой причине нам необходимо передавать
в него тольео корутины.

if _name_=='_main_':
  task=[main(link) for link in urls]
  for x in task:
    print(type(x))

  asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
  asyncio.run(asyncio.wait(task))

Результат:
  <class 'coroutine'>
  <class 'coroutine'>
  ...
  <class 'coroutine'>
  <class 'coroutine'>
- Функция asyncio.wait(), о которой мы говорили в предыдущих разделах, одновременно 
запускает awaitable-объекты из указанного нами списка и блокирует программу до ее завершения.

4. Результат:
  - Если вы запускали код у себя в терминале, то обратили внмиание на то, что ссылки в консоль 
печатаются в случайном порядке. Случайный порядок возвращаемых результатов обусловлен спецификой
работы цикла событий. Как только ответ с данными от сервера получен, он тут же возвращается
и его результат печатается.

Приготовление асинхронного супа часть 2

В этой части мы разберем на практике как динамически получать разделы и пагинацию и работать со
всем этим асинхронно. Сразу хочу предупредить, что все примеры в этом степе будут написаны 
в простом функциональном стиле, хотя для этого идеально подходит ОПП. Это сделано для того,
чтобы вам не пришлось дополнительно разбираться в ООП, а если вы уже знаете его, то для вас
не будет сложности переписать этот код так, как вы привыкли. Все-таки, подавляющее 
большинство учеников курса - начинающие, и не очень хочется еще сильнее усложнять и без того
сложную тему.

Этот код с docstring можно посмотреть на github.com (https://github.com/nefelsay/stepik_parsing/blob/main/create_async_soup.py)

Этот код уже содержит в себе динамический сбор страниц пагинации в каждой категории. Тут у нас
есть три синхронные функции, которые нам помогают это сделать, но их мы не будем запускать
в цикле событий.

import aiohttp
import asyncio
import requests
from bs4 import BeautifulSoup

category_lst = []
pagen_lst = []
domain = 'https://parsinger.ru/html/'

def get_soup(url):
  resp = requests.get(url=url)
  return BeautifulSoup(resp.text, 'lxml')

def get_urls_categories(soup):
  all_link = soup.find('div', class_='nav_menu').find_all('a')

  for cat in all_link:
    category_lst.append(domain+cat['href'])

def get_urls_pages(category_lst):
  for cat in category_lst:
    resp = requests.get(url=cat)
    soup = BeautifulSoup(resp.text, 'lxml')
    for pagen in soup.find('div', class_='pagen').find_all('a'):
      pagen_lst.append(domain+pagen['href'])

async def get_data(session, link):
  async with session.get(url=link) as response:
    resp = await response.text()
    soup = BeautifulSoup(resp, 'lxml')
    item_card = [x['href'] for x in soup.find_all('a', class_='name_item')]
    for x in item_card:
      url2 = domain + x
      async with session.get(url=url2) as response2:
        resp2 = await response2.text()
        soup2 = BeautifulSoup(resp2, 'lxml')
        article = soup2.find('p', class_='article').text
        name = soup2.find('p', id='p_header').text
        price = soup2.find('span', id='price').text
        print(url2, price, article, name)

async def main():
  async with aiohttp.ClientSession() as session:
    tasks = []
    for link in pagen_lst:
      task = asyncio.create_task(get_data(session, link))
      tasks.append(task)
    await asyncio.gather(*tasks)

url = 'https://parsinger.ru/html/index1_page_1.html'
soup = get_soup(url)
get_urls_categories(soup)
get_urls_pages(category_lst)

asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
asyncio.run(main())

Итак, давайте подробнее поговорим про каждую из функций, сверху-вниз.

1. Функция def get_soup(url):
  def get_soup(url):
    resp = requests.get(url=url)
    return BeautifulSoup(rexp.text, 'lxml')

Эта функция максимально проста, онна извлекает суп из переданной в нее первый ссылки на сайте
https://parsinger.ru/html/index1_page_1.html

2. Функция get_urls_categories(soup):

  def get_urls_categories(soup):
    all_link = soup.find('div', class_='nav_menu').find_all('a')
    for cat in all_link:
      category_lst.append(domain+cat['href'])

Эта функция извлекает все ссылки из правого списка с навигацией, в которой находятся все
категории товаров, также она наполняет глобальный список category_lst = [] ссылками всех
категорий. На нашем сайте - тренажере всего пять категорий, и все пять ссылок попадают в этот
глобальный список. Он нам потребуется для извлесчения длины пагинации в каждой из этих 
категорий.

category_lst = ['https://parsinger.ru/html/index1_page_1.html',
                'https://parsinger.ru/html/index2_page_1.html',
                'https://parsinger.ru/html/index3_page_1.html',
                'https://parsinger.ru/html/index4_page_1.html',
                'https://parsinger.ru/html/index5_page_1.html',]

3. Функция def get_urls_pages(categor_lst):

Эта функция собирает все ссылки пагинации, которые находятся в каждой из категорий. Также эта
функция формирует глобальный список pagen_lst = [], на каждой из этих ссылок находится по восемь
карточек с товаром, из которых мы будем извлекать данные в асинхронных функциях.

pagen_lst = ['https://parsinger.ru/html/index1_page_1.html',
              'https://parsinger.ru/html/index1_page_2.html',
              'https://parsinger.ru/html/index1_page_3.html',
              'https://parsinger.ru/html/index1_page_4.html',
              'https://parsinger.ru/html/index2_page_1.html',
              'https://parsinger.ru/html/index2_page_2.html',
              'https://parsinger.ru/html/index2_page_3.html',
              'https://parsinger.ru/html/index2_page_4.html',
              'https://parsinger.ru/html/index3_page_1.html',
              'https://parsinger.ru/html/index3_page_2.html',
              'https://parsinger.ru/html/index3_page_3.html',
              'https://parsinger.ru/html/index3_page_4.html',
              'https://parsinger.ru/html/index4_page_1.html',
              'https://parsinger.ru/html/index4_page_2.html',
              'https://parsinger.ru/html/index4_page_3.html',
              'https://parsinger.ru/html/index4_page_4.html',
              'https://parsinger.ru/html/index5_page_1.html',
              'https://parsinger.ru/html/index5_page_2.html',
              'https://parsinger.ru/html/index5_page_3.html',
              'https://parsinger.ru/html/index5_page_4.html',]

4. Асинхронная функция async def main():

В этой функции происходит создание session при помощи библиотеки aiohttp, также в этой 
функции мы создаем таски (задачи) task = asyncio.create_task(get_data(session, link)),
в каждую задачу мы заворачиваем корутину get_data(session, link) и передаем в нее созданную
ранее session и ссылку, колученную из глобального списка pagen_lst. Когда все задачи 
сформированы и список tasks наполнен, мы передаем распокованный список tasks в функцию 
.gather(*tasks), чтобы она собрала все задачи и передала их в цикл событий. Каждый из тасков
(задач) имеет тип <class '_asyncio.Task'>. Каждый task является awaitable-объектом который
подходит для их передачи в функцию .gather(*tasks) для дальнейшей отправки в цикл событий.

5. Функция async def get_data(session, link):

В этой функции происходит извлечение данных с каждой страницы. Эта функция получает на вход 
session and link из глобального списка со ссылками pagen_lst = []. В этой функции мы работаем
с BeautifulSoup точно так же, как и работали бы при написании синхронного парсера. Здесь нам 
пришлось использовать session дважды, для более глубокого извлечения данных с карточек.
Аналогичный код мы писали в задачах по сбору данных со всех 160 карточек с товаром. 
В асинхронном коде такой подход тоже допустим, т.к. мы используем одну session для всех наших
запросов. Пример ниже наглядно продемонстрирует множественное переиспользование сессии
для многоуровневого извлечения данных. Пример синтетический, но хорошо демонстрирует
возможности глубины парсинга. Попробуйте понять логику этого кода, онна не очень сложная, но
понимание этого облегчит написание собственных парсеров.

import aiohttp
import asyncio
from bs4 import BeautifulSoup

async def main(url):
  async with aiohttp.ClientSession() as session:
    async with session.get(url=url) as response:
      resp = await response.text()
      soup = BeautifulSoup(resp, 'lxml')
      item_card = [x['href'] for x in soup.find_all('a', class_='link')]
      for url2 in item_card:
        async with session.get(url=url2) as response2:
          resp2 = await response2.text()
          soup2 = BeautifulSoup(resp2, 'lxml')
          item_card2 = [x['href'] for x in soup2.find_all('a', class_ = 'link2')]
          for url3 in item_card2:
            async with session.get(url=url3) as response3:
              resp3 = await response3.text()
              soup3 = BeautifulSoup(resp3, 'lxml')
              item_card3 = [x['href'] for x in soup3.find_all('a', class_='link3')]
              for url4 in item_card3:
                async with session.get(url=url4) as response4:
                  resp4 = await response4.text()
                  soup4 = BeautifulSoup(resp4, 'lxml')
                  data = [x['href'] for x in soup4.find_all('div', class_='data')]
                  print(data)

asyncio.set_event_loop_policy(asyncio.WindowsSeletorEventLoopPolicy())
asyncio.run(main('http://example.com'))

Aiohttp-retry или Повтор неуспешного подключения

При создании асинхронного парсинга вы получаете не только высокую скорость сбора данных, но и 
повышенную вероятность получить бан по IP. Как работать с прокси, мы подробно разобрали в прошлом 
модуле. Также нередки случаи когда сервер закрывает одно из ваших асинхронных подключений, это чаще
всего происходит из-за большого количества запросов. Это еще не бан, но запрос сервером обработан не был.
Для повторного запроса в случае неудачи существует отличный клиент, который работает совместно с aiohttp
и называется aiohttp-retry.

aiohttp-retry документация - https://github.com/inyutin/aiohttp_retry

Установите этот модуль себе в окружающую среду вашего проекта, он нам понадобится для запуска кода.

Установка
pip install aiohttp-retry

Импорт
from aiohttp-retry import RetryClient

Самый простой пример из документации. Но у такого примера есть один недостаток: по умолчанию будет 
производиться всего лишь один повторный запрос. В случае, если и второй запрос окажется неудачным, вы
не получите требуемые данные. Мне пришлось немного модифицировать пример из документации, мы ведь
любим пользоваться менеджером контекста with/as. В этом случае нам не нужно закрывать вручную нашу
сессию командой =>

await client_sesison.close()

import aiohttp
from aiohttp_retry import RetryClient

async def main():
  async with aiohttp.ClientSession() as session:
    client_session = session
    retry_client = RetryClient(client_session=client_session)
    async with retry_client.get('https://ya.ru') as response:
      print(response.status)

Для того, чтобы у нас была возможность управлять количеством повторных подключений, нам потребуется
использовать класс ExponenetialRetry из модуля aiohttp_retry. Импортируем этот класс из модуля и немного
модифицируем первый пример.

import aiohttp
from aiohttp_retry import RetryClient, ExponentialRetry

async def get_data(session, link):
  retry_options = ExponentialRetry(attemts=10)
  retry_client = RetryClient(raise_for_status=False, retry_options=retry_options, client_session=session)
  async with retry_client.get(link) as response:

Давайте разбираться, что тут написано и как с этим работать.

1. retry_options = ExponentialRetry(attempts=10) - создаем экземпляр класса ExponentialRetry и передаем
атрибут attempts=, этот атрибут принимает целое число, которое указывает на количество повторных попыток
подключения. В данном примере будет произведено десять попыток повторных подключений. Ниже указаны все
доступные атрибуты.
- ExponentialRetry(attempts=10) - (int) количество повторный подключений;
- ExponentialRetry(atart_timeout=0.5) - (float) базовое время ожидания, с каждым новым запросом
значение возрастает экспоненционально;
- ExponentialRetry(max_timeput=30.0) - (float) максимально возможное время ожидания;
- ExponentialRetry(factor=3.0) - (float) - на переданное значение будет увеличиваться timeout
ожидания для следующего запроса;
- ExponentialRetry(statuses=[400,403]) - list(int, int) на каких статусах нужно повторить запрос;
- ExponentialRetry(exceptions=[Exeption, TimeoutError]) - list(type_Exception, type_Exception) - 
при каких исключениях повторять запрос, по умочанию None;
- ExponentialRetry(retry_all_server_errors=True) - повторить подключение при любых ошибках сервера,
явно указывать не нужно, т.к. по умолчанию True.

2. retry_client = RetryClient(raise_for_status=False, retry_options=retry_options,client_session=session)
создаем экземпляр класса RetryClient и в атрибутах передаём следующие параметры.
- RetryClient(client_sesison = session) - передаёт объект сессии которую мы получили от aiohttp;
- RetryClient(retry_options=retry_options) - передает экземпляр класса Exponentialretry(),в котором
мы настраивали параметры повторного запроса.
- RetryClient(raise_for_status=True) - Если True, использует время ответа сервера в качестве 
параметра для расчета следующего timeout, лучше использовать параметр False, который установлен по
умолчанию. Если True, вы можете столкнуться с проблемой, когда сервер не ответил и следующий timeout
получит значение None, тогда расчет времени для следующего запроса, будет  не рассчитан, потому
что из None невозможно ничего рассчитать;
- RetryClient(logger=[type_logger]) - устанавливает логирование запросов, подробнее можно почитать
в документации; (https://github.com/inyutin/aiohttp_retry)

Эталонный асинхронный парсер

К этому этапу курса вы уже умеете работать с заголовками запроса, умете использовать fake_useragent,
также умеете работать с прокси и модулем повторных запросов. Давайте теперь напишем эталонный 
асинхронный парсер, который собирает данные с сайта-тренажера, с применением полученных знаний.

Стек знаний, которые будут использованы в коде, представлены ниже:

1. Библиотека asyncio;
2. Библиотека aiohttp;
3. Подключим прокси через ProxyConnector;
4. ИСпользует бибилиотеку fake_useragent для подмену нашего user_agent;
5. Ну и конечно же применим aiohttp-retry;

Попробуйте самостоятельно разобраться как работает код ниже. Если вам что-то не понятно, задайте ваш
вопрос в комменатриях и я постараюсь вам ответить как можно скорее. Все понятно и вопросов нет?
Напишите аналогичный парсер для нашего сайта-тренажёра, (https://parsinger.ru/html/index1_page_1.html),
используя этот код как шпаргалку.

Для запуска кода ниже потребуется хотя-бы один рабочий прокси.

Для запуска этого когда потребуются рабочие прокси.


